{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a name=\"table-of-contents\"></a>\n",
    "# Protein Localization Prediction using Kolmogorov-Arnold Networks.\n",
    "Github: https://github.com/JinMaxx/Protein-Localization-using-KANs<br>\n",
    "For optimal performance, please use a GPU runtime T4 or better.\n",
    "\n",
    "## Table of Contents\n",
    "- [01. Install pip Dependencies (Must Execute)](#1-install-pip-dependencies-must-execute)\n",
    "- [02. Check Files (Must Execute)](#2-check-files-must-execute)\n",
    "- [03. Settings (Must execute)](#3-settings-must-execute)\n",
    "- [04. Create Encodings](#4-create-encodings)\n",
    "- [05. Visualize Data](#5-visualize-data)\n",
    "- [06. Train Model](#6-train-model)\n",
    "- [07. Continue Training a Saved Model](#7-continue-training-a-saved-model)\n",
    "- [08. Model Comparison](#8-model-comparison)\n",
    "- [09. Explore Figures](#9-explore-figures)\n",
    "\n",
    "---\n",
    "\n",
    "License: MIT<br>\n",
    "This notebook and its code are made available under the MIT License.<br>\n",
    "See [https://opensource.org/licenses/MIT](https://opensource.org/licenses/MIT) for details.<br>\n",
    "\n",
    "---"
   ],
   "id": "d08ba5ed85a7db5d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a name=\"1-install-pip-dependencies-must-execute\"></a>\n",
    "### 1. Install pip Dependencies (Must Execute)\n",
    "#### [↑](#table-of-contents) [→](#2-check-files-must-execute)"
   ],
   "id": "d4ebe25907c58584"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!git clone https://github.com/JinMaxx/Protein-Localization-using-KANs.git /content/project_root\n",
    "\n",
    "!pip install -q \\\n",
    "    optuna \\\n",
    "    dotenv \\\n",
    "    pyfaidx \\\n",
    "    colorcet \\\n",
    "    biopython \\\n",
    "    umap_learn \\\n",
    "    transformers \\\n",
    "    sentencepiece \\\n",
    "    typing-extensions \\\n",
    "    kaleido==0.2.1 \\\n",
    "    plotly==5.5.0 \\\n",
    "    git+https://github.com/AthanasiosDelis/faster-kan.git \\\n",
    "    \"huggingface_hub[hf_xet]\"  # to ignore missing account warnings\n",
    "\n",
    "# git+https://github.com/ZiyaoLi/fast-kan.git\n",
    "\n",
    "\n",
    "%load_ext rpy2.ipython\n",
    "\n",
    "__cell1 = True"
   ],
   "id": "ac3b59b339f0d1ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a name=\"2-check-files-must-execute\"></a>\n",
    "### 2. Check Files (Must Execute)\n",
    "#### [←](#1-install-pip-dependencies-must-execute) [↑](#table-of-contents) [→](#3-settings-must-execute)"
   ],
   "id": "33705631b3ecdc49"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "assert '__cell1' in globals(), \"You must execute cell 1 before!\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import sysconfig\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- Project Path Setup ---\n",
    "# The root path of the project is the directory of the cloned the project repository\n",
    "__project_root_path = \"/content/project_root\"\n",
    "os.chdir(__project_root_path)\n",
    "if __project_root_path not in sys.path:\n",
    "    sys.path.append(__project_root_path)\n",
    "\n",
    "# --- Environment Variables ---\n",
    "__dotenv_path = os.path.join(__project_root_path, \".env\")\n",
    "if os.path.exists(__dotenv_path):\n",
    "    load_dotenv(dotenv_path=__dotenv_path)\n",
    "    print(\"Successfully loaded environment variables from .env file.\")\n",
    "else:\n",
    "    raise FileNotFoundError(\".env file not found in the repository root.\")\n",
    "\n",
    "# For compatibility with subsequent cells, set BASE_COLAB programmatically\n",
    "os.environ['BASE_COLAB'] = __project_root_path\n",
    "\n",
    "\n",
    "# --- GPU Check ---\n",
    "# for google colab checking if running with GPU\n",
    "__gpu_info = !nvidia-smi\n",
    "__gpu_info = '\\n'.join(__gpu_info)\n",
    "if __gpu_info.find('failed') >= 0: print('Not connected to a GPU')\n",
    "else: print(__gpu_info)\n",
    "\n",
    "\n",
    "# --- File Listing (for verification) ---\n",
    "def _list_files(path) -> list[str] | None:\n",
    "    # Check if the folder exists and list files\n",
    "    if os.path.exists(path):\n",
    "        files = os.listdir(path)  # List all files and directories\n",
    "        if files:\n",
    "            print(f\"Files and directories in '{path}':\")\n",
    "            for file in files: print(f\"- {file}\")\n",
    "            return files\n",
    "        else:\n",
    "            print(f\"The folder '{path}' is empty.\")\n",
    "            return []\n",
    "    else:\n",
    "        print(f\"The folder '{path}' does not exist.\")\n",
    "        return None\n",
    "\n",
    "_list_files(__project_root_path)\n",
    "\n",
    "\n",
    "sys.path.append(__project_root_path)  # Project root\n",
    "\n",
    "\n",
    "# --- APPLY PATCHES ---\n",
    "\n",
    "patches_to_apply = [\n",
    "    # {\n",
    "    #     \"target_rel_path\": os.path.join(\"<PACKAGE_DIR_NAME>\", \"<FILE>.py\"),\n",
    "    #     \"patch_file_path\": \"source/patches/<FILE>.patch\",\n",
    "    #     \"description\": \"<PACKAGE> <FILE>.py\"\n",
    "    # },\n",
    "]\n",
    "\n",
    "if patches_to_apply:\n",
    "    print(\"\\n--- Applying patches ---\")\n",
    "\n",
    "    # Find the site-packages directory once\n",
    "    site_packages_path = sysconfig.get_path('purelib')\n",
    "    print(f\"Located site-packages directory at: {site_packages_path}\")\n",
    "\n",
    "    for patch_info in patches_to_apply:\n",
    "        description = patch_info[\"description\"]\n",
    "        target_file = os.path.join(site_packages_path, patch_info[\"target_rel_path\"])\n",
    "        patch_file = patch_info[\"patch_file_path\"]\n",
    "\n",
    "        print(f\"\\nAttempting to patch {description}...\")\n",
    "\n",
    "        # Check that both files exist before attempting the patch\n",
    "        if os.path.exists(target_file) and os.path.exists(patch_file):\n",
    "            print(f\"  - Found target file: {target_file}\")\n",
    "            print(f\"  - Found patch file: {patch_file}\")\n",
    "\n",
    "            # Execute the patch command using shell access\n",
    "            !patch \"{target_file}\" < \"{patch_file}\"\n",
    "\n",
    "            print(f\"  => Patch for {description} applied successfully.\")\n",
    "        else:\n",
    "            print(f\"  => ERROR: Patch for {description} failed.\")\n",
    "            if not os.path.exists(target_file):\n",
    "                print(f\"    - Target file not found at: {target_file}\")\n",
    "            if not os.path.exists(patch_file):\n",
    "                print(f\"    - Patch file not found at: {patch_file}\")\n",
    "\n",
    "    print(\"\\n--- Finished applying all patches ---\")\n",
    "    # --- END OF PATCH SECTION ---\n",
    "\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "__cell2 = True"
   ],
   "id": "fb637a292bac844b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a name=\"3-settings-must-execute\"></a>\n",
    "### 3. Settings (Must execute)\n",
    "#### [←](#2-check-files-must-execute) [↑](#table-of-contents) [→](#4-create-encodings)"
   ],
   "id": "351de552ef646f21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "assert '__cell2' in globals(), \"You must execute cell 2 before!\"\n",
    "\n",
    "# @markdown Choose a protein language model:\n",
    "encoding_model_name = \"Rostlab/prot_t5_xl_uniref50\" # @param [\"onehot\", \"Rostlab/prot_t5_xl_uniref50\", \"Rostlab/prot_t5_xl_half_uniref50-enc\", \"ElnaggarLab/ankh-base\", \"ElnaggarLab/ankh-large\", \"facebook/esm2_t6_8M_UR50D\", \"facebook/esm2_t12_35M_UR50D\", \"facebook/esm2_t30_150M_UR50D\", \"facebook/esm2_t33_650M_UR50D\"]\n",
    "\n",
    "_fasta_input_dir: str      = os.getenv(\"ENCODINGS_INPUT_DIR_COLAB\")\n",
    "_encodings_output_dir: str = os.path.join(os.getenv('ENCODINGS_OUTPUT_DIR_COLAB'), encoding_model_name)\n",
    "\n",
    "train_file_name = \"deeploc_our_train_set\"  # @param {type:\"string\"}\n",
    "val_file_name   = \"deeploc_our_val_set\"    # @param {type:\"string\"}\n",
    "test_file_name  = \"setHARD\"                # @param {type:\"string\"}\n",
    "                # \"deeploc_test_set\"\n",
    "\n",
    "_train_encodings_file_path = os.path.join(\n",
    "    _encodings_output_dir,\n",
    "    f\"{os.path.splitext(os.path.basename(train_file_name))[0]}.h5\"\n",
    ")\n",
    "_val_encodings_file_path   = os.path.join(\n",
    "    _encodings_output_dir,\n",
    "    f\"{os.path.splitext(os.path.basename(val_file_name))[0]}.h5\"\n",
    ")\n",
    "_test_encodings_file_path  = os.path.join(\n",
    "    _encodings_output_dir,\n",
    "    f\"{os.path.splitext(os.path.basename(test_file_name))[0]}.h5\"\n",
    ")\n",
    "\n",
    "\n",
    "# Setting to more specified subfolders corresponding to encoding model. -> Less confusion and mistakes\n",
    "\n",
    "_model_save_dir: str                = os.path.join(os.getenv(\"MODEL_SAVE_DIR_COLAB\"), encoding_model_name)\n",
    "_figures_save_dir: str              = os.path.join(os.getenv(\"FIGURES_SAVE_DIR_COLAB\"), encoding_model_name)\n",
    "_studies_save_dir: str              = os.path.join(os.getenv(\"STUDIES_SAVE_DIR_COLAB\"), encoding_model_name)\n",
    "\n",
    "_log_file_path: str                 = os.getenv(\"LOG_FILE_PATH_COLAB\")\n",
    "_training_metrics_file_path: str    = os.getenv(\"TRAINING_METRICS_FILE_PATH_COLAB\")\n",
    "_hyper_param_metrics_file_path: str = os.getenv(\"HYPER_PARAM_METRICS_FILE_PATH_COLAB\")\n",
    "\n",
    "_log_file_path = os.path.join(os.path.dirname(_log_file_path), encoding_model_name, os.path.basename(_log_file_path))\n",
    "_training_metrics_file_path = os.path.join(os.path.dirname(_training_metrics_file_path), encoding_model_name, os.path.basename(_training_metrics_file_path))\n",
    "_hyper_param_metrics_file_path = os.path.join(os.path.dirname(_hyper_param_metrics_file_path), encoding_model_name, os.path.basename(_hyper_param_metrics_file_path))\n",
    "\n",
    "\n",
    "print(f\"fasta_input_dir: ............. {_fasta_input_dir}\")\n",
    "if not os.path.isdir(_fasta_input_dir): raise FileNotFoundError(f\"Input directory {_fasta_input_dir} does not exist.\")\n",
    "if not _list_files(_fasta_input_dir): raise FileNotFoundError(f\"Input directory {_fasta_input_dir} is empty.\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(f\"encodings_output_dir: ........ {_encodings_output_dir}\\n\")\n",
    "os.makedirs(_encodings_output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"train_encodings_file_path: ... {_train_encodings_file_path}\")\n",
    "print(f\"val_encodings_file_path: ..... {_val_encodings_file_path}\")\n",
    "print(f\"test_encodings_file_path: .... {_test_encodings_file_path}\\n\")\n",
    "\n",
    "print(f\"model_save_dir: .............. {_model_save_dir}\")\n",
    "os.makedirs(_encodings_output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"figures_save_dir: ............ {_figures_save_dir}\")\n",
    "os.makedirs(_encodings_output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"studies_save_dir: ............ {_studies_save_dir}\")\n",
    "os.makedirs(_encodings_output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"log_file_path: ............... {_log_file_path}\")\n",
    "print(f\"training_metrics_file_path: .. {_training_metrics_file_path}\")\n",
    "print(f\"hyper_param_metrics_file_path: {_hyper_param_metrics_file_path}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# @markdown Edit config.yaml?\n",
    "edit_config = False  # @param {type:\"boolean\"}\n",
    "\n",
    "__config_file_path = os.getenv(\"CONFIG_PATH_COLAB\")\n",
    "\n",
    "if edit_config:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, Markdown\n",
    "\n",
    "    __content: str\n",
    "    with open(__config_file_path, \"r\") as file:\n",
    "        __content = file.read()\n",
    "\n",
    "    __textarea = widgets.Textarea(\n",
    "        value = __content,\n",
    "        layout = widgets.Layout(width='100%', height='400px')\n",
    "    )\n",
    "    __save_button = widgets.Button(description=\"Save\", button_style='success')\n",
    "    __output = widgets.Output()\n",
    "\n",
    "    def save_config(_):\n",
    "        with __output:\n",
    "            # output.clear_output()\n",
    "            try:\n",
    "                with open(__config_file_path, 'w') as file:\n",
    "                    file.write(__textarea.value)\n",
    "                print(\"Configuration saved!\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "\n",
    "    __save_button.on_click(save_config)\n",
    "\n",
    "    display(Markdown(\"**Edit your config below:**\"))\n",
    "    display(__textarea)\n",
    "    display(__save_button, __output)\n",
    "\n",
    "else:\n",
    "    from IPython.display import display, Markdown\n",
    "\n",
    "    with open(__config_file_path, \"r\") as file:\n",
    "        __content = file.read()\n",
    "        display(Markdown(f\"```yaml\\n{__content}\\n```\"))\n",
    "\n",
    "\n",
    "__cell3 = True"
   ],
   "id": "d80fc874e78dc958",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a name=\"4-create-encodings\"></a>\n",
    "### 4. Create Encodings\n",
    "#### [←](#3-settings-must-execute) [↑](#table-of-contents) [→](#5-visualize-data)"
   ],
   "id": "c381f63bf13cae85"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "assert '__cell3' in globals(), \"You must execute cell 3 before!\"\n",
    "\n",
    "import time\n",
    "from source.data_scripts.encodings import main as generate_embeddings\n",
    "\n",
    "# @markdown Choose batch size (might impact memory):\n",
    "batch_size = 18  # @param {type:\"slider\", min:1, max:64, step:1}\n",
    "# reaching ~10GB GPU Memory Peaks\n",
    "\n",
    "# @markdown Choose number of threads (ignored if CUDA/GPU availible)\n",
    "threads = 8  # @param {type:\"slider\", min:1, max:16, step:1}\n",
    "\n",
    "# @markdown Automatically terminate colab runtime:\n",
    "terminate = True  # @param {type:\"boolean\"}\n",
    "\n",
    "if not _list_files(_fasta_input_dir):\n",
    "    raise FileNotFoundError(\"Input directory is empty or does not exist.\")\n",
    "\n",
    "\n",
    "try:\n",
    "    await generate_embeddings(\n",
    "        model_name = encoding_model_name,\n",
    "        input_dir = _fasta_input_dir,\n",
    "        output_dir = _encodings_output_dir,\n",
    "        batch_size = batch_size,\n",
    "        threads = threads\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"Error during encoding generation:\")\n",
    "    print(str(e))\n",
    "    terminate = True\n",
    "\n",
    "time.sleep(60)  # \"Wait a minute.\" - Kazoo Kid\n",
    "# Prematurely terminating would make some file transfers to drive incomplete.\n",
    "\n",
    "if terminate:\n",
    "  from google.colab import runtime\n",
    "  runtime.unassign()"
   ],
   "id": "776552ed2d0da0a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a name=\"5-visualize-data\"></a>\n",
    "### 5. Visualize Data\n",
    "#### [←](#4-create-encodings) [↑](#table-of-contents) [→](#6-train-model)"
   ],
   "id": "16b6bb4f10740174"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "assert '__cell3' in globals(), \"You must execute cell 3 before!\"\n",
    "\n",
    "from source.data_scripts.data_figures import DataFiguresCollection, PoolingType\n",
    "from source.data_scripts.encodings import load_metadata_from_hdf5, stream_seq_enc_data_from_hdf5\n",
    "\n",
    "\n",
    "if not _list_files(_encodings_output_dir):\n",
    "    raise FileNotFoundError(\"Input directory is empty or does not exist.\")\n",
    "\n",
    "# @markdown Automatically terminate colab runtime:\n",
    "terminate = True  # @param {type:\"boolean\"}\n",
    "\n",
    "\n",
    "__figures: DataFiguresCollection = DataFiguresCollection(save_dir=_figures_save_dir)\n",
    "__figures.class_distribution()\n",
    "__figures.pca_embedding(pooling_type=PoolingType.Per_Protein_Mean)\n",
    "__figures.tsne_embedding(pooling_type=PoolingType.Per_Protein_Mean)\n",
    "__figures.umap_embedding(pooling_type=PoolingType.Per_Protein_Mean)\n",
    "__figures.pca_embedding(pooling_type=PoolingType.Per_Protein_Max)\n",
    "__figures.tsne_embedding(pooling_type=PoolingType.Per_Protein_Max)\n",
    "__figures.umap_embedding(pooling_type=PoolingType.Per_Protein_Max)\n",
    "__figures.raw_sequence_length_distribution(bins=150, log_y=False)\n",
    "__figures.embedding_length_distribution(bins=150, log_y=False)\n",
    "__figures.embedding_length_distribution(bins=150, log_y=True)\n",
    "# __figures.pairwise_distance_distribution(bins=150, distance_metric=\"euclidean\", pooling_type=__pooling_type)\n",
    "# Simply too many pairwise comparisons resulting in too many values for the figure to handle...\n",
    "\n",
    "\n",
    "for __file_name, __file_path in [\n",
    "    (train_file_name, _train_encodings_file_path),\n",
    "    (val_file_name,   _val_encodings_file_path),\n",
    "    (test_file_name,  _test_encodings_file_path),\n",
    "    (\"Aggregated\", [_train_encodings_file_path, _val_encodings_file_path, _test_encodings_file_path])\n",
    "]:\n",
    "\n",
    "    __encoding_dim, __count, __label_count = load_metadata_from_hdf5(file_path=__file_path)\n",
    "    __seq_enc_data_generator_supplier = lambda: stream_seq_enc_data_from_hdf5(file_path=__file_path)\n",
    "\n",
    "    __identifier = f\"{encoding_model_name}_{__file_name}\"\n",
    "\n",
    "    __figures.update(\n",
    "        identifier = __identifier.replace(\"/\", \"_\"),\n",
    "        label_count = __label_count,\n",
    "        seq_enc_data_generator_supplier = __seq_enc_data_generator_supplier,\n",
    "        file_name = __file_name,\n",
    "        encoding_model = encoding_model_name,\n",
    "    )\n",
    "\n",
    "    __figures.save(\"data_visualization\")\n",
    "\n",
    "    print(f\"{__identifier}\")\n",
    "    print(f\"Encoding dimension: {__encoding_dim}\")\n",
    "    print(f\"Number of sequences: {__count}\")\n",
    "\n",
    "del __figures\n",
    "\n",
    "\n",
    "if terminate:\n",
    "  from google.colab import runtime\n",
    "  runtime.unassign()"
   ],
   "id": "defbf9447eaff282",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a name=\"6-train-model\"></a>\n",
    "### 6. Train Model\n",
    "#### [←](#5-visualize-data) [↑](#table-of-contents) [→](#7-continue-training-a-saved-model)"
   ],
   "id": "ee80078a82cda2b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "assert '__cell3' in globals(), \"You must execute cell 3 before!\"\n",
    "\n",
    "from source.metrics.metrics import Metrics\n",
    "from source.models.abstract import AbstractModel\n",
    "from source.training.utils.save_state import SaveState\n",
    "\n",
    "from source.models.ffn import MLP, MLPpp, FastKAN\n",
    "from source.models.other.attention_lstm_hybrid import AttentionLstmHybrid\n",
    "from source.models.other.lstm_reduction_hybrid import LstmReductionHybrid\n",
    "from source.models.other.light_attention import (\n",
    "    LightAttention,\n",
    "    LightAttentionFastKAN,\n",
    ")\n",
    "from source.models.reduced_ffn import (\n",
    "    MaxPoolFastKAN,\n",
    "    MaxPoolMLP,\n",
    "    AvgPoolFastKAN,\n",
    "    AvgPoolMLP,\n",
    "    LinearFastKAN,\n",
    "    LinearMLP,\n",
    "    AttentionFastKAN,\n",
    "    AttentionMLP,\n",
    "    PositionalFastKAN,\n",
    "    PositionalMLP,\n",
    "    UNetFastKAN,\n",
    "    UNetMLP\n",
    ")\n",
    "\n",
    "\n",
    "__model_name_to_class: dict[str, AbstractModel] = {\n",
    "    \"MLP\": MLP,\n",
    "    \"MLP_PerProtein\": MLPpp,\n",
    "    \"FastKAN\": FastKAN,\n",
    "\n",
    "    \"LightAttention\": LightAttention,\n",
    "    \"LightAttentionFastKAN\": LightAttentionFastKAN,\n",
    "\n",
    "    \"MaxPoolFastKAN\": MaxPoolFastKAN,\n",
    "    \"MaxPoolMLP\": MaxPoolMLP,\n",
    "    \"AvgPoolFastKAN\": AvgPoolFastKAN,\n",
    "    \"AvgPoolMLP\": AvgPoolMLP,\n",
    "    \"LinearFastKAN\": LinearFastKAN,\n",
    "    \"LinearMLP\": LinearMLP,\n",
    "    \"AttentionFastKAN\": AttentionFastKAN,\n",
    "    \"AttentionMLP\": AttentionMLP,\n",
    "    \"PosFastKAN\": PositionalFastKAN,\n",
    "    \"PosMLP\": PositionalMLP,\n",
    "    \"UNetFastKAN\": UNetFastKAN,\n",
    "    \"UNetMLP\": UNetMLP,\n",
    "\n",
    "    \"AttentionLstmHybrid\": AttentionLstmHybrid,\n",
    "    \"LstmReductionHybrid\": LstmReductionHybrid\n",
    "}\n",
    "\n",
    "# @ Select Model to train (adjust parameters in config.yaml):\n",
    "model_name = \"AttentionFastKAN\"  # @param [\"MLP\", \"MLP_PerProtein\", \"FastKAN\", \"MaxPoolFastKAN\", \"MaxPoolMLP\", \"AvgPoolFastKAN\", \"AvgPoolMLP\", \"LinearFastKAN\", \"LinearMLP\", \"AttentionFastKAN\", \"AttentionMLP\", \"PosFastKAN\", \"PosMLP\", \"UNetFastKAN\", \"UNetMLP\", \"AttentionLstmHybrid\", \"LstmReductionHybrid\", \"LightAttention\", \"LightAttentionFastKAN\"]\n",
    "\n",
    "use_config_defaults = True  # @param {type:\"boolean\"}\n",
    "\n",
    "# @ Settings predefined for hyper parameter tuning\n",
    "epochs = 20  # @param {type:\"slider\", min:1, max:100, step:1}\n",
    "# consider higher patience. Sometimes model recalibrate themselves after overfitting on epoch 3\n",
    "# Recommended: patience 8 for hyper param tuning\n",
    "patience = 14  # @param {type:\"slider\", min:1, max:20, step:1}\n",
    "batch_size = 28  # @param {type:\"slider\", min:1, max:64, step:1}\n",
    "\n",
    "learning_rate = 0.00005  # @param [\"0.001\", \"0.0005\", \"0.0001\", \"0.00005\"] {\"type\":\"raw\"}\n",
    "learning_rate_decay = 0.99  # @param [\"0.80\", \"0.85\", \"0.90\", \"0.95\", \"0.975\", \"0.99\", \"0.999\", \"1.0\"] {\"type\":\"raw\"}\n",
    "use_weights = False  # @param {type:\"boolean\"}\n",
    "weight_decay = 0.0001  # @param [\"0.001\", \"0.0005\", \"0.0001\", \"0.00005\", \"0.00001\"] {\"type\":\"raw\"}\n",
    "\n",
    "# @markdown Select if you want to perform a hyperparameter search or just train the model:\n",
    "do_hyper_param_search = False  # @param {type:\"boolean\"}\n",
    "\n",
    "# @markdown Automatically terminate colab runtime:\n",
    "terminate = True  # @param {type:\"boolean\"}\n",
    "\n",
    "if not _list_files(_encodings_output_dir):\n",
    "    raise FileNotFoundError(f\"Encodings directory {_encodings_output_dir} is empty or does not exist.\")\n",
    "\n",
    "__model_class = __model_name_to_class[model_name]\n",
    "\n",
    "if use_config_defaults:\n",
    "    from source.config import TrainingConfig, ConfigType, parse_config\n",
    "    training_config: TrainingConfig = parse_config(ConfigType.Training)\n",
    "    epochs = training_config.epochs\n",
    "    patience = training_config.patience\n",
    "    batch_size = training_config.batch_size\n",
    "    learning_rate = training_config.learning_rate\n",
    "    learning_rate_decay = training_config.learning_rate_decay\n",
    "    use_weights = training_config.use_weights\n",
    "    weight_decay = training_config.weight_decay\n",
    "    print(f\"Loaded parameters: epochs={epochs}, patience={patience}, batch_size={batch_size}, learning_rate={learning_rate}, learning_rate_decay={learning_rate_decay}, use_weights={use_weights}, weight_decay={weight_decay}\")\n",
    "else:\n",
    "    print(f\"Using manually defined parameters: epochs={epochs}, patience={patience}, batch_size={batch_size}, learning_rate={learning_rate}, learning_rate_decay={learning_rate_decay}, use_weights={use_weights}, weight_decay={weight_decay}\")\n",
    "\n",
    "\n",
    "try:\n",
    "\n",
    "    if do_hyper_param_search:\n",
    "        from source.training.hyper_param import main as tune\n",
    "\n",
    "        __n_trials = 5\n",
    "        __timeout = None\n",
    "\n",
    "            tune(\n",
    "                model_class = __model_class,\n",
    "                train_encodings_file_path = _train_encodings_file_path,\n",
    "                val_encodings_file_path = _val_encodings_file_path,\n",
    "                epochs = epochs,\n",
    "                patience = patience,\n",
    "                batch_size = batch_size,\n",
    "                use_weights = use_weights,\n",
    "                weight_decay = weight_decay,\n",
    "                learning_rate = learning_rate,\n",
    "                learning_rate_decay = learning_rate_decay,\n",
    "                n_trials = __n_trials,\n",
    "                timeout = __timeout,\n",
    "                study_name = __model_class.name(),\n",
    "                studies_save_dir = _studies_save_dir,\n",
    "                model_save_dir = _model_save_dir,\n",
    "                figures_save_dir = _figures_save_dir,\n",
    "                metrics_file_path= _hyper_param_metrics_file_path,\n",
    "                log_file_path = _log_file_path\n",
    "            )\n",
    "\n",
    "    else:\n",
    "        from source.training.train_model import main as train\n",
    "        train(\n",
    "            model = __model_class,\n",
    "            train_encodings_file_path = _train_encodings_file_path,\n",
    "            val_encodings_file_path = _val_encodings_file_path,\n",
    "            epochs = epochs,\n",
    "            patience = patience,\n",
    "            batch_size = batch_size,\n",
    "            use_weights = use_weights,\n",
    "            weight_decay = weight_decay,\n",
    "            learning_rate = learning_rate,\n",
    "            learning_rate_decay = learning_rate_decay,\n",
    "            model_save_dir = _model_save_dir,\n",
    "            figures_save_dir = _figures_save_dir,\n",
    "            metrics_file_path= _training_metrics_file_path,\n",
    "            log_file_path = _log_file_path\n",
    "        )\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error during training:\")\n",
    "    print(str(e))\n",
    "    raise e\n",
    "    terminate = True\n",
    "\n",
    "\n",
    "_list_files(_model_save_dir)\n",
    "\n",
    "\n",
    "if terminate:\n",
    "  from google.colab import runtime\n",
    "  runtime.unassign()"
   ],
   "id": "a6ce17bbd471ea70",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a name=\"7-continue-training-a-saved-model\"></a>\n",
    "### 7. Continue Training a Saved Model\n",
    "#### [←](#6-train-model) [↑](#table-of-contents) [→](#8-model-comparison)"
   ],
   "id": "a5e742b8cf721746"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "assert '__cell3' in globals(), \"You must execute cell 3 before!\"\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from source.training.train_model import main as train\n",
    "from source.training.utils.save_state import SaveState\n",
    "\n",
    "\n",
    "use_config_defaults = True  # @param {type:\"boolean\"}\n",
    "epochs = 150  # @param {type:\"slider\", min:1, max:250, step:1}\n",
    "# consider higher patience. Sometimes model recalibrate themselves after overfitting on epoch 3\n",
    "patience = 25  # @param {type:\"slider\", min:1, max:50, step:1}\n",
    "batch_size = 28  # @param {type:\"slider\", min:1, max:64, step:1}\n",
    "\n",
    "learning_rate = 0.0005  # @param [\"0.001\", \"0.005\", \"0.001\", \"0.0005\", \"0.0001\"] {\"type\":\"raw\"}\n",
    "learning_rate_decay = 0.95  # @param [\"0.80\", \"0.85\", \"0.90\", \"0.95\", \"0.975\", \"0.99\"] {\"type\":\"raw\"}\n",
    "\n",
    "use_weights = True  # @param {type:\"boolean\"}\n",
    "weight_decay = 0.0001 # @param [\"0.001\", \"0.0005\", \"0.0001\", \"0.00005\", \"0.00001\"] {\"type\":\"raw\"}\n",
    "\n",
    "# @markdown Automatically terminate colab runtime:\n",
    "terminate = True  # @param {type:\"boolean\"}\n",
    "\n",
    "\n",
    "if use_config_defaults:\n",
    "    from source.config import TrainingConfig, ConfigType, parse_config\n",
    "    training_config: TrainingConfig = parse_config(ConfigType.Training)\n",
    "    epochs = training_config.epochs\n",
    "    patience = training_config.patience\n",
    "    batch_size = training_config.batch_size\n",
    "    learning_rate = training_config.learning_rate\n",
    "    learning_rate_decay = training_config.learning_rate_decay\n",
    "    use_weights = training_config.use_weights\n",
    "    weight_decay = training_config.weight_decay\n",
    "    print(f\"Loaded parameters: epochs={epochs}, patience={patience}, batch_size={batch_size}, learning_rate={learning_rate}, learning_rate_decay={learning_rate_decay}, use_weights={use_weights}, weight_decay={weight_decay}\")\n",
    "else:\n",
    "    print(f\"Using manually defined parameters: epochs={epochs}, patience={patience}, batch_size={batch_size}, learning_rate={learning_rate}, learning_rate_decay={learning_rate_decay}, use_weights={use_weights}, weight_decay={weight_decay}\")\n",
    "\n",
    "\n",
    "__files = _list_files(_model_save_dir)\n",
    "# __files.sort()\n",
    "if not __files: raise FileNotFoundError(f\"Model save directory {_model_save_dir} is empty or does not exist.\")\n",
    "\n",
    "\n",
    "def __train(selected_filename: str):\n",
    "\n",
    "    try:\n",
    "        train(\n",
    "            model = os.path.join(_model_save_dir, selected_filename),\n",
    "            train_encodings_file_path = _train_encodings_file_path,\n",
    "            val_encodings_file_path = _val_encodings_file_path,\n",
    "            epochs = epochs,\n",
    "            patience = patience,\n",
    "            batch_size = batch_size,\n",
    "            use_weights = use_weights,\n",
    "            weight_decay = weight_decay,\n",
    "            learning_rate = learning_rate,\n",
    "            learning_rate_decay = learning_rate_decay,\n",
    "            model_save_dir = _model_save_dir,\n",
    "            figures_save_dir = _figures_save_dir,\n",
    "            log_file_path = _log_file_path\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error during continued training:\")\n",
    "        print(str(e))\n",
    "        raise e\n",
    "        terminate = True\n",
    "\n",
    "    finally:\n",
    "        if terminate:\n",
    "            from google.colab import runtime\n",
    "            runtime.unassign()\n",
    "\n",
    "\n",
    "__dropdown = widgets.Dropdown(\n",
    "    options = __files,\n",
    "    description = 'Select:',\n",
    "    value = None\n",
    ")\n",
    "__button = widgets.Button(description=\"Continue\")\n",
    "\n",
    "def __on_button_clicked(b):\n",
    "    if __dropdown.value:\n",
    "        print(f\"Model selected: {__dropdown.value}\")\n",
    "        __train(__dropdown.value)\n",
    "    else: print(\"No model selected!\")\n",
    "\n",
    "__button.on_click(__on_button_clicked)\n",
    "display(__dropdown, __button)"
   ],
   "id": "a09c38739cd7d27e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a name=\"8-model-comparison\"></a>\n",
    "### 8. Model Comparison\n",
    "#### [←](#7-continue-training-a-saved-model) [↑](#table-of-contents) [→](#9-explore-figures)"
   ],
   "id": "2dc86ce86d673a2c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "assert '__cell3' in globals(), \"You must execute cell 3 before!\"\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from typing_extensions import List\n",
    "from source.metrics.metrics import Metrics\n",
    "from source.models.abstract import AbstractModel\n",
    "from source.evaluation.evaluation import evaluate_models\n",
    "\n",
    "\n",
    "iterations = 100  # @param {type:\"slider\", min:10, max:1000, step:10}\n",
    "batch_size = 28  # @param {type:\"slider\", min:1, max:64, step:1}\n",
    "\n",
    "# # @markdown Automatically terminate colab runtime:\n",
    "# terminate = True  # @param {type:\"boolean\"}\n",
    "\n",
    "\n",
    "__model_file_names: List[str] = _list_files(_model_save_dir)\n",
    "\n",
    "if not __model_file_names:\n",
    "    raise FileNotFoundError(f\"No models found in directory: {_model_save_dir}\")\n",
    "\n",
    "\n",
    "__selection = widgets.SelectMultiple(\n",
    "    options = __model_file_names,\n",
    "    # value=tuple(__model_file_names),  # pre-select all\n",
    "    description = 'Models',\n",
    "    disabled = False,\n",
    "    layout = widgets.Layout(width='100%')\n",
    ")\n",
    "\n",
    "__button = widgets.Button(description=\"Load Selected Models\")\n",
    "\n",
    "# __output = widgets.Output()\n",
    "\n",
    "def on_button_clicked(_):\n",
    "    __output.clear_output()\n",
    "    # with __output:\n",
    "    model_file_paths = [\n",
    "        os.path.join(_model_save_dir, model_file_name)\n",
    "        for model_file_name in __selection.value\n",
    "    ]\n",
    "    print(\"Model selected:\")\n",
    "    for model_file_path in model_file_paths: print(model_file_path)\n",
    "    try:\n",
    "        evaluate_models(\n",
    "            model_file_paths = model_file_paths,\n",
    "            test_encodings_file_path = _test_encodings_file_path,\n",
    "            figures_save_dir = _figures_save_dir,\n",
    "            iterations = iterations,\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"Error during model comparison:\")\n",
    "        print(str(e))\n",
    "        raise e\n",
    "        terminate = True\n",
    "\n",
    "__button.on_click(on_button_clicked)\n",
    "\n",
    "display(widgets.VBox([__selection, __button])) #, __output]))"
   ],
   "id": "5b003911e1661d40",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a name=\"9-explore-figures\"></a>\n",
    "### 9. Explore Figures\n",
    "#### [←](#8-model-comparison) [↑](#table-of-contents)"
   ],
   "id": "df2d4192df8488f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "assert '__cell3' in globals(), \"You must execute cell 3 before!\"\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from typing_extensions import List\n",
    "from IPython.display import display\n",
    "from source.abstract_figures import AbstractFiguresCollection\n",
    "\n",
    "\n",
    "__figures: AbstractFiguresCollection = AbstractFiguresCollection()\n",
    "\n",
    "__figures_file_names: List[str] = [\n",
    "    __file_name\n",
    "    for __file_name in _list_files(_figures_save_dir)\n",
    "    if __file_name.lower().endswith(\".pkl\")\n",
    "]\n",
    "\n",
    "if not __figures_file_names:\n",
    "    raise FileNotFoundError(f\"No figures found in directory: {_figures_save_dir}\")\n",
    "\n",
    "\n",
    "__selection = widgets.SelectMultiple(\n",
    "    options = __figures_file_names,\n",
    "    # value=tuple(__figures_file_names),  # pre-select all\n",
    "    description = 'Figures',\n",
    "    disabled = False,\n",
    "    layout = widgets.Layout(width='100%')\n",
    ")\n",
    "\n",
    "__button = widgets.Button(description=\"Load Selected Figures\")\n",
    "\n",
    "# __output = widgets.Output()\n",
    "\n",
    "def on_button_clicked(_):\n",
    "    # __output.clear_output()\n",
    "    # with __output:\n",
    "    for file_name in __selection.value:\n",
    "        __figures.load(os.path.join(_figures_save_dir, file_name))\n",
    "    print(f\"Loaded and displayed {len(__selection.value)} figures.\")\n",
    "    __figures.display()\n",
    "\n",
    "__button.on_click(on_button_clicked)\n",
    "\n",
    "display(widgets.VBox([__selection, __button])) #, __output]))"
   ],
   "id": "2419bd55fe9e7e56",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
