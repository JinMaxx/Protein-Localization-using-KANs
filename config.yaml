# ----------------- Python Module Config ------------------

# similar to the paper: "Light attention predicts protein location from the language of life"
# From comparison with the paper and the code from train.py
#Training:
#  epochs: 2500 # Not seen in the paper but: --num_epochs default=2500
#  patience: 80 # For early stopping as stated in paper.
#  batch_size: 28 # Lower so my GPU Memory can handle my data and models.
#  l2_penalty: 0.0 # They certainly did not use weight decay, but if so, it is done via --optimizer_parameters
#  weight_factor: 0.0 # No class weights. # They might have used weights but probably not as it is: --balanced_loss default=False
#  learning_rate: 0.00005 # As stated in the paper.
#  learning_rate_decay: 1.0 # Constant learning rate as stated in the paper.


Training:
  epochs: 50 # Training is quite slow for me and this is a reasonable value. Most models converge at epoch 50
  patience: 20 # Reasonable for me as 20 epochs with not even a minimal increase is not worth waiting longer. Overfitting happens usually afterward.
  batch_size: 28 # Lower so my GPU Memory can handle my data and models.
  l2_penalty: 0.0 # Consider using a slight l2 penalty of 0.0001 as most models overfit slightly with optimized hyperparameters.
  weight_factor: 0.0 # No class weights. Consider a small weight factor of 0.2 for imbalanced data.
  learning_rate: 0.00005 # As stated in the paper.
  learning_rate_decay: 0.98  # Slight learning rate decay.


HyperParam:
  epochs: 20
  patience: 14
  batch_size: 28
  l2_penalty: 0.0
  weight_factor: 0.0 # Consider a small weight factor of 0.3.
  learning_rate: 0.00005
  learning_rate_decay: 0.98

  n_trials: 10  # trials
  timeout: # 14400  # 4-hour max: 60 sec/min * 60 min/hour * 4


Encodings:
  threads: 8
  batch_size: 16  # 32


# High weights for accuracy, f1, auc_pr to handle imbalanced data.
Metrics:
  accuracy_weight: 8  # consider increasing
  sensitivity_weight: 7
  specificity_weight: 5
  precision_weight: 6
  f1_score_weight: 8
  auc_roc_weight: 4
  auc_pr_weight: 9


Evaluation:

  batch_size: 28
  iterations: 100

  # From "Light attention predicts protein location from the language of life"
  # https://pmc.ncbi.nlm.nih.gov/articles/PMC9710637/
  # Because I compare only the LightAttention Model as baseline, the others are not relevant.

  model_names:
  #  - UniRep-FNN
  #  - SeqVec-FNN
  #  - ProtBert-FNN
  #  - ESM-1b-FNN
  #  - ProtT5-FNN
  #  - UniRep-LA
  #  - SeqVec-LA
  #  - ProtBert-LA
  #  - ESM-1b-LA
    - ProtT5-LA

  # deeploc_test_set
  # accuracies:
  #   - 0.68
  #   - 0.71
  #   - 0.76
  #   - 0.80
  #   - 0.82
  #   - 0.71
  #   - 0.76
  #   - 0.80
  #   - 0.83
  #   - 0.86

  # setHARD
  accuracies:
  #  - 0.49
  #  - 0.52
  #  - 0.53
  #  - 0.60
  #  - 0.61
  #  - 0.55
  #  - 0.57
  #  - 0.58
  #  - 0.62
    - 0.65

  # I can't read the values of the error bars from the figure.
  # And I can't find the file protein-localization/data/results/paper_tables.CSV
  # ./data is set in their .gitignore, so they have not pushed that file.
  accuracies_errors:
  #  - .nan
  #  - .nan
  #  - .nan
  #  - .nan
  #  - .nan
  #  - .nan
  #  - .nan
  #  - .nan
  #  - .nan
    - .nan



# -------------------- ML Model Config --------------------

# -------- Basic FFN configs (used for debugging) ---------

FastKAN: # input layer size per residue is 70 * 1024 = 71_680

  in_seq_len: 20  # truncated (or padded) to the first 20 Encodings (should correlate to aminoacids)
  hidden_layers: !HiddenLayersRelative
    num_layers: 1  # small model with one hidden layer for debugging
    relative_size: 0.06
  grid_diff: 2.0
  num_grids: 7

  test_parameters:
    hidden_layers: !HiddenLayers
      num_layers: !RangeInt
        min: 1
        max: 2
      layer_size_percentage: !RangeFloat
        min: 0.06
        max: 0.08
        step: 0.01
    grid_diff: !RangeFloat
      min: 1.5
      max: 2.5
      step: 0.5
    num_grids: !RangeInt
      min: 4
      max: 8
      step: 1


MLP:

  in_seq_len: 20
  hidden_layers: !HiddenLayersRelative
    num_layers: 2
    relative_size: 0.4

  test_parameters:
    hidden_layers: !HiddenLayers
      num_layers: !RangeInt
        min: 1
        max: 5
      layer_size_percentage: !RangeFloat
        min: 0.3
        max: 0.8
        step: 0.05


MLP_Per_Protein:

  in_seq_len: 1024  # Dimension of ProtT5
  hidden_layers: !HiddenLayersRelative
    num_layers: 2
    relative_size: 0.4

  test_parameters:
    hidden_layers: !HiddenLayers
      num_layers: !RangeInt
        min: 1
        max: 5
      layer_size_percentage: !RangeFloat
        min: 0.3
        max: 0.8
        step: 0.05


LightAttentionFFN:

  in_seq_len: 20  # Not the actual in_seq_len for LightAttention but for a standalone FFN to debug.
  hidden_layers: !HiddenLayersExact
    - 32
  dropout_rate: 0.25

  test_parameters:
    hidden_layers: !HiddenLayers
      num_layers: !RangeInt
        min: 1
        max: 3
      layer_size_percentage: !RangeFloat
        min: 0.05
        max: 0.6
        step: 0.05


# -------------------- Reduced FFN Models --------------------

MaxPoolFastKAN:

  in_seq_len: 1023 # * embeddings_dim=1024 = 1_047_552 (too much that's why dimension reduction)
  hidden_layers: !HiddenLayersRelative
    num_layers: 1
    relative_size: 0.04
  reduced_seq_len: 20
  grid_diff: 2.0
  num_grids: 5

  test_parameters:
    hidden_layers: !HiddenLayers
      num_layers: !RangeInt
        min: 1
        max: 3
      layer_size_percentage: !RangeFloat
        min: 0.02
        max: 0.24
        step: 0.02
    reduced_seq_len: !RangeInt
      min: 10
      max: 60
      step: 10
    grid_diff: !RangeFloat
      min: 1.0
      max: 3.0
      step: 0.5
    num_grids: !RangeInt
      min: 3
      max: 6
      step: 1


MaxPoolMLP:

  in_seq_len: 1023
  hidden_layers: !HiddenLayersRelative
    num_layers: 2
    relative_size: 0.4
  reduced_seq_len: 20

  test_parameters:
    hidden_layers: !HiddenLayers
      num_layers: !RangeInt
        min: 1
        max: 6
      layer_size_percentage: !RangeFloat
        min: 0.30
        max: 0.80
        step: 0.10
    reduced_seq_len: !RangeInt
      min: 10
      max: 60
      step: 10


AvgPoolFastKAN:

  in_seq_len: 1023
  hidden_layers: !HiddenLayersRelative
    num_layers: 1
    relative_size: 0.04
  reduced_seq_len: 20
  grid_diff: 2.0
  num_grids: 5

  test_parameters:
    hidden_layers: !HiddenLayers
      num_layers: !RangeInt
        min: 1
        max: 3
      layer_size_percentage: !RangeFloat
        min: 0.02
        max: 0.24
        step: 0.02
    reduced_seq_len: !RangeInt
      min: 10
      max: 60
      step: 10
    grid_diff: !RangeFloat
      min: 1.0
      max: 3.0
      step: 0.5
    num_grids: !RangeInt
      min: 3
      max: 6
      step: 1


AvgPoolMLP:

  in_seq_len: 1023
  hidden_layers: !HiddenLayersRelative
    num_layers: 2
    relative_size: 0.4
  reduced_seq_len: 20

  test_parameters:
    hidden_layers: !HiddenLayers
      num_layers: !RangeInt
        min: 1
        max: 6
      layer_size_percentage: !RangeFloat
        min: 0.30
        max: 0.80
        step: 0.10
    reduced_seq_len: !RangeInt
      min: 10
      max: 60
      step: 10


LinearFastKAN:

  in_seq_len: 1023
  hidden_layers: !HiddenLayersRelative
    num_layers: 1
    relative_size: 0.04
  reduced_seq_len: 20
  grid_diff: 2.0
  num_grids: 5

  test_parameters:
    hidden_layers: !HiddenLayers
      num_layers: !RangeInt
        min: 1
        max: 3
      layer_size_percentage: !RangeFloat
        min: 0.02
        max: 0.24
        step: 0.02
    reduced_seq_len: !RangeInt
      min: 10
      max: 60
      step: 10
    grid_diff: !RangeFloat
      min: 1.0
      max: 3.0
      step: 0.5
    num_grids: !RangeInt
      min: 3
      max: 6
      step: 1


LinearMLP:

  in_seq_len: 1023
  hidden_layers: !HiddenLayersRelative
    num_layers: 2
    relative_size: 0.4
  reduced_seq_len: 10

  test_parameters:
    hidden_layers: !HiddenLayers
      num_layers: !RangeInt
        min: 1
        max: 6
      layer_size_percentage: !RangeFloat
        min: 0.30
        max: 0.80
        step: 0.10
    reduced_seq_len: !RangeInt
      min: 10
      max: 60
      step: 10


AttentionFastKAN:

  in_seq_len: 1023
  hidden_layers: !HiddenLayersRelative
    num_layers: 1
    relative_size: 0.04
  reduced_seq_len: 10
  grid_diff: 1.5
  num_grids: 7
  num_heads: 8

  # 2de816ef-8c09-4628-8930-52e3fe969738
  # 67.64%
  #- in_channels: 1024
  #- in_seq_len: 1023
  #- out_channels: 10
  #- reduced_seq_len: 10
  #- hidden_layers: HiddenLayers(kind=Type.RELATIVE, value=(0.04, 1))
  #- num_heads: 8
  #- grid_diff: 1.5
  #- num_grids: 7

  test_parameters:
    hidden_layers: !HiddenLayers
      num_layers: !RangeInt
        min: 1
        max: 3
      layer_size_percentage: !RangeFloat
        min: 0.02
        max: 0.24
        step: 0.02
    reduced_seq_len: !RangeInt
      min: 10
      max: 40
      step: 10
    num_heads: !Categorical
      choices: [ 4, 8, 16 ]
    grid_diff: !RangeFloat
      min: 0.5
      max: 3.0
      step: 0.5
    num_grids: !RangeInt
      min: 3
      max: 6
      step: 1


AttentionMLP:

  in_seq_len: 1023
  hidden_layers: !HiddenLayersRelative
    num_layers: 3
    relative_size: 0.4
  reduced_seq_len: 10
  num_heads: 8

  # 71020308-7a83-46a5-872c-9768ece46f49
  # 64.11%
  #- in_channels: 1024
  #- in_seq_len: 1023
  #- out_channels: 10
  #- reduced_seq_len: 10
  #- hidden_layers: HiddenLayers(kind=Type.RELATIVE, value=(0.4, 3))
  #- num_heads: 8

  test_parameters:
    hidden_layers: !HiddenLayers
      num_layers: !RangeInt
        min: 1
        max: 6
      layer_size_percentage: !RangeFloat
        min: 0.30
        max: 0.80
        step: 0.10
    reduced_seq_len: !RangeInt
      min: 10
      max: 60
      step: 10
    num_heads: !Categorical
      choices: [ 4, 8, 16 ]


PositionalFastKAN:
  # todo find flaw

  in_seq_len: 1023
  hidden_layers: !HiddenLayersRelative
    num_layers: 1
    relative_size: 0.14
  reduced_seq_len: 10
  reduced_channels: 512
  kernel_size: 3
  weights_scalar: 2.0
  grid_diff: 2.0
  num_grids: 6

  # 1c5a
  # 61.1%

  test_parameters:
    hidden_layers: !HiddenLayers
      num_layers: !RangeInt
        min: 1
        max: 3
      layer_size_percentage: !RangeFloat
        min: 0.02
        max: 0.24
        step: 0.02
    reduced_seq_len: !RangeInt
      min: 10
      max: 60
      step: 10
    reduced_channels: !RangeInt
      min: 256
      max: 1024
      step: 64
    kernel_size: !Categorical
      choices: [ 3, 5, 7 ]
    weights_scalar: !RangeFloat
      min: 1.0
      max: 5.0
      step: 1.0
    grid_diff: !RangeFloat
      min: 1.0
      max: 3.0
      step: 0.5
    num_grids: !RangeInt
      min: 3
      max: 6
      step: 1


PositionalMLP:

  in_seq_len: 1023
  hidden_layers: !HiddenLayersRelative
    num_layers: 2
    relative_size: 0.5
  reduced_seq_len: 10
  reduced_channels: 512
  kernel_size: 3
  weights_scalar: 2.0

  # 1f52f9ad-de0c-4ac9-9c37-90cf42e85468
  # 64.4%
  #- in_channels: 1024
  #- in_seq_len: 1023
  #- out_channels: 10
  #- reduced_seq_len: 10
  #- reduced_channels: 512
  #- hidden_layers: HiddenLayers(kind=Type.RELATIVE, value=(0.5, 2))
  #- kernel_size: 3
  #- weights_scalar: 2.0

  test_parameters:
    hidden_layers: !HiddenLayers
      num_layers: !RangeInt
        min: 1
        max: 6
      layer_size_percentage: !RangeFloat
        min: 0.30
        max: 0.80
        step: 0.10
    reduced_seq_len: !RangeInt
      min: 10
      max: 60
      step: 10
    reduced_channels: !RangeInt
      min: 256
      max: 1024
      step: 64
    kernel_size: !Categorical
      choices: [3, 5, 7]
    weights_scalar: !RangeFloat
      min: 1.0
      max: 5.0
      step: 1.0


UNetFastKAN:

  in_seq_len: 1023
  hidden_layers: !HiddenLayersRelative
    num_layers: 2
    relative_size: 0.16
  reduced_seq_len: 10
  reduced_channels: 960
  kernel_size: 3
  grid_diff: 1.0
  num_grids: 8

  # f4c2340a-94be-4547-b5b3-ad2f19a9c633
  # 63.66
  #- in_channels: 1024
  #- in_seq_len: 1023
  #- out_channels: 10
  #- reduced_seq_len: 10
  #- reduced_channels: 960
  #- hidden_layers: HiddenLayers(kind=Type.RELATIVE, value=(0.16, 2))
  #- kernel_size: 3
  #- grid_diff: 1.0
  #- num_grids: 8

  test_parameters:
    hidden_layers: !HiddenLayers
      num_layers: !RangeInt
        min: 1
        max: 3
      layer_size_percentage: !RangeFloat
        min: 0.02
        max: 0.24
        step: 0.02
    reduced_seq_len: !RangeInt
      min: 10
      max: 60
      step: 10
    reduced_channels: !RangeInt
      min: 256
      max: 1024
      step: 64
    kernel_size: !Categorical
      choices: [ 3, 5, 7 ]
    grid_diff: !RangeFloat
      min: 1.0
      max: 3.0
      step: 0.5
    num_grids: !RangeInt
      min: 3
      max: 6
      step: 1


UNetMLP:

  in_seq_len: 1023
  hidden_layers: !HiddenLayersRelative
    num_layers: 2
    relative_size: 0.8
  reduced_seq_len: 10
  reduced_channels: 960
  kernel_size: 3

  # UNetMLP_d465 63%
  #
  #    in_seq_len: 1023
  #    hidden_layer_relative: 0.8
  #    num_hidden_layers: 2
  #    hidden_layer_exact: None
  #    reduced_seq_len: 20
  #    reduced_channels: 512

  # 89b1e76d-c68d-4d20-8b8a-5d9ed3e1267a
  # 62.56%
  #- in_channels: 1024
  #- in_seq_len: 1023
  #- out_channels: 10
  #- reduced_seq_len: 10
  #- reduced_channels: 960
  #- hidden_layers: HiddenLayers(kind=Type.RELATIVE, value=(0.8, 2))
  #- kernel_size: 3

  test_parameters:
    hidden_layers: !HiddenLayers
      num_layers: !RangeInt
        min: 1
        max: 6
      layer_size_percentage: !RangeFloat
        min: 0.30
        max: 0.80
        step: 0.10
    reduced_seq_len: !RangeInt
      min: 10
      max: 60
      step: 10
    reduced_channels: !RangeInt
      min: 256
      max: 1024
      step: 64
    kernel_size: !Categorical
      choices: [3, 5, 7]


# -------------------- Other Models --------------------

AttentionLstmHybridFastKAN:

  in_seq_len: 1023 # Input sequence length from ProtT5 embeddings
  hidden_layers: !HiddenLayersRelative
    num_layers: 1
    relative_size: 0.04
  attention_num_heads: 8
  lstm_hidden_size: 256
  lstm_num_layers: 2
  dropout1_rate: 0.25
  dropout2_rate: 0.25
  grid_diff: 1.0
  num_grids: 8

  # 59f12f39-ce40-4edc-bd54-a372ec0c9702
  # 62.68
  #- in_channels: 1024
  #- in_seq_len: 1023
  #- out_channels: 10
  #- attention_num_heads: 8
  #- lstm_hidden_size: 256
  #- lstm_num_layers: 2
  #- dropout1_rate: 0.25
  #- dropout2_rate: 0.25
  #- hidden_layers: HiddenLayers(kind=Type.RELATIVE, value=(0.04, 1))
  #- grid_diff: 1.0
  #- num_grids: 8

  test_parameters:
    hidden_layers: !HiddenLayers
      num_layers: !RangeInt
        min: 1
        max: 3
      layer_size_percentage: !RangeFloat
        min: 0.02
        max: 0.24
        step: 0.02
    attention_num_heads: !Categorical
      choices: [4, 8, 16]
    lstm_hidden_size: !RangeInt
      min: 128
      max: 512
      step: 32
    lstm_num_layers: !RangeInt
      min: 1
      max: 4
    grid_diff: !RangeFloat
      min: 1.0
      max: 3.0
      step: 0.5
    num_grids: !RangeInt
      min: 4
      max: 6
      step: 1


LstmAttentionReductionHybridFastKAN:

  in_seq_len: 1023 # Input sequence length from ProtT5 embeddings
  hidden_layers: !HiddenLayersRelative
    num_layers: 1
    relative_size: 0.04
  reduced_seq_len: 10
  lstm_hidden_size: 128
  lstm_num_layers: 2
  dropout_rate: 0.25
  num_heads: 8
  grid_diff: 1.0
  num_grids: 8

  # dcac07f3-37b2-4c8c-9268-c91d595b4e56
  # 61.84%
  #- in_channels: 1024
  #- in_seq_len: 1023
  #- out_channels: 10
  #- reduced_seq_len: 10
  #- lstm_hidden_size: 128
  #- lstm_num_layers: 2
  #- dropout_rate: 0.25
  #- hidden_layers: HiddenLayers(kind=Type.RELATIVE, value=(0.04, 1))
  #- num_heads: 8
  #- grid_diff: 1.0
  #- num_grids: 8

  test_parameters:
    hidden_layers: !HiddenLayers
      num_layers: !RangeInt
        min: 1
        max: 3
      layer_size_percentage: !RangeFloat
        min: 0.02
        max: 0.24
        step: 0.02
    reduced_seq_len: !RangeInt
      min: 10
      max: 60
      step: 10
    lstm_hidden_size: !RangeInt
      min: 128
      max: 512
      step: 32
    lstm_num_layers: !RangeInt
      min: 1
      max: 4
    dropout_rate: !RangeFloat
      min: 0.1
      max: 0.5
    num_heads: !Categorical
      choices: [ 4, 8, 16 ]
    grid_diff: !RangeFloat
      min: 1.0
      max: 3.0
      step: 0.5
    num_grids: !RangeInt
      min: 4
      max: 6
      step: 1


LightAttention:
  # Configuration exactly as described in the paper "Light attention predicts protein location from the language of life".

  in_seq_len: 1023  # Input sequence length from ProtT5 embeddings
  hidden_layers: !HiddenLayersExact
    - 32
  kernel_size: 9
  conv_dropout_rate: 0.25
  ffn_dropout_rate: 0.25

  # beb27502-bd1c-443b-bc3d-81a8f93897b0
  # 64.41%
  #- in_channels: 1024
  #- in_seq_len: 1023
  #- out_channels: 10
  #- kernel_size: 9
  #- conv_dropout_rate: 0.25
  #- hidden_layers: HiddenLayers(kind=Type.EXACT, value=[32])
  #- ffn_dropout_rate: 0.25

  test_parameters:
    hidden_layers: !HiddenLayers
      num_layers: !RangeInt
        min: 1
        max: 4
      layer_size_percentage: !RangeFloat
        min: 0.2
        max: 0.6
        step: 0.1
    kernel_size: !RangeInt
      min: 3
      max: 11
      step: 2
    conv_dropout_rate: !RangeFloat
      min: 0.1
      max: 0.5
      step: 0.1
    ffn_dropout_rate: !RangeFloat
      min: 0.1
      max: 0.5
      step: 0.1


LightAttentionFastKAN:
  # Settings similar to LightAttention (for comparison)

  in_seq_len: 1023  # Input sequence length from ProtT5 embeddings
  hidden_layers: !HiddenLayersExact
    - 32 # Going from 2048 (2 * input_channels) to 32 is a massive reduction.
  kernel_size: 9
  conv_dropout_rate: 0.25
  grid_diff: 1.0  # smaller window for less noise
  num_grids: 8  # expressive activation function

  # c4cd8511-823d-4829-984c-2a8641f543c1
  # 65.01%
  #- in_channels: 1024
  #- in_seq_len: 1023
  #- out_channels: 10
  #- kernel_size: 9
  #- conv_dropout_rate: 0.25
  #- hidden_layers: HiddenLayers(kind=Type.EXACT, value=[32])
  #- grid_diff: 1.0
  #- num_grids: 8

  test_parameters:
    hidden_layers: !HiddenLayers
      num_layers: !RangeInt
        min: 1
        max: 3
      layer_size_percentage: !RangeFloat
        min: 0.02
        max: 0.24
        step: 0.02
    kernel_size: !RangeInt
      min: 3
      max: 11
      step: 2
    conv_dropout_rate: !RangeFloat
      min: 0.1
      max: 0.5
    grid_diff: !RangeFloat
      min: 1.0
      max: 3.0
      step: 0.5
    num_grids: !RangeInt
      min: 4
      max: 6
      step: 1