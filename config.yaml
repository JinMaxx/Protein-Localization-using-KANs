# ----------------- Python Module Config ------------------

# similar to the paper: "Light attention predicts protein location from the language of life"
# From comparison with the paper and the code from train.py
#Training:
#  epochs: 2500 # Not seen in the paper but: --num_epochs default=2500
#  patience: 80 # For early stopping as stated in paper.
#  batch_size: 28 # Lower so my GPU Memory can handle my data and models.
#  use_weights: false # They might have used weights but probably not as it is: --balanced_loss default=False
#  weight_decay: 0.0 # They certainly did not use weight decay, but if so, it is done via --optimizer_parameters
#  learning_rate: 0.00005 # As stated in the paper.
#  learning_rate_decay: 1.0 # Constant learning rate as stated in the paper.


Training:
  epochs: 50 # Training is quite slow for me and this is a reasonable value. Most models converge at epoch 50.
  patience: 20 # Reasonable for me as 20 epochs with not even a minimal increase is not worth waiting longer. Overfitting happens usually afterward.
  batch_size: 28 # Lower so my GPU Memory can handle my data and models.
  use_weights: false # They might have used weights but probably not as it is: --balanced_loss default=False
  weight_decay: 0.0 # They certainly did not use weight decay, but if so, it is done via --optimizer_parameters
  learning_rate: 0.00005 # As stated in the paper.
  learning_rate_decay: 0.98 # A decent gentle learning rate decay.


# TODO: this config has no real effect
HyperParam:
  n_trials: 10  # trials
  timeout: 14400  # 4-hour max: 60 sec/min * 60 min/hour * 4


Encodings:
  threads: 8
  batch_size: 16  # 32


# High weights for accuracy, f1, auc_pr to handle imbalanced data.
Metrics:
  accuracy_weight: 8  # consider increasing
  sensitivity_weight: 7
  specificity_weight: 5
  precision_weight: 6
  f1_score_weight: 8
  auc_roc_weight: 4
  auc_pr_weight: 9


Evaluation:

  batch_size: 28
  iterations: 100

  # From "Light attention predicts protein location from the language of life"
  # https://pmc.ncbi.nlm.nih.gov/articles/PMC9710637/
  # Because I compare only the LightAttention Model as baseline, the others are not relevant.

  model_names:
  #  - UniRep-FNN
  #  - SeqVec-FNN
  #  - ProtBert-FNN
  #  - ESM-1b-FNN
  #  - ProtT5-FNN
  #  - UniRep-LA
  #  - SeqVec-LA
  #  - ProtBert-LA
  #  - ESM-1b-LA
    - ProtT5-LA

  # deeploc_test_set
  # accuracies:
  #   - 0.68
  #   - 0.71
  #   - 0.76
  #   - 0.80
  #   - 0.82
  #   - 0.71
  #   - 0.76
  #   - 0.80
  #   - 0.83
  #   - 0.86

  # setHARD
  accuracies:
  #  - 0.49
  #  - 0.52
  #  - 0.53
  #  - 0.60
  #  - 0.61
  #  - 0.55
  #  - 0.57
  #  - 0.58
  #  - 0.62
    - 0.65

  # I can't read the values of the error bars from the figure.
  # And I can't find the file protein-localization/data/results/paper_tables.CSV
  # ./data is set in their .gitignore, so they have not pushed that file.
  accuracies_errors:
  #  - .nan
  #  - .nan
  #  - .nan
  #  - .nan
  #  - .nan
  #  - .nan
  #  - .nan
  #  - .nan
  #  - .nan
    - .nan



# -------------------- ML Model Config --------------------

# -------- Basic FFN configs (used for debugging) ---------

FastKAN: # input layer size per residue is 70 * 1024 = 71_680

  in_seq_len: 20  # truncated (or padded) to the first 20 Encodings (should correlate to aminoacids)
  hidden_layers: !HiddenLayersRelative
    num_layers: 1  # small model with one hidden layer for debugging
    relative_size: 0.06
  grid_diff: 2.0
  num_grids: 7

  test_parameters:
    hidden_layers: !HiddenLayers
      num_layers: !RangeInt
        min: 1
        max: 2
      layer_size_percentage: !RangeFloat
        min: 0.06
        max: 0.08
        step: 0.01
    grid_diff: !RangeFloat
      min: 1.5
      max: 2.5
      step: 0.5
    num_grids: !RangeInt
      min: 4
      max: 8
      step: 1


MLP:

  in_seq_len: 20
  hidden_layers: !HiddenLayersRelative
    num_layers: 2
    relative_size: 0.4

  test_parameters:
    hidden_layers: !HiddenLayers
      num_layers: !RangeInt
        min: 1
        max: 5
      layer_size_percentage: !RangeFloat
        min: 0.3
        max: 0.8
        step: 0.05


MLP_Per_Protein:

  in_seq_len: 1024  # Dimension of ProtT5
  hidden_layers: !HiddenLayersRelative
    num_layers: 2
    relative_size: 0.4

  test_parameters:
    hidden_layers: !HiddenLayers
      num_layers: !RangeInt
        min: 1
        max: 5
      layer_size_percentage: !RangeFloat
        min: 0.3
        max: 0.8
        step: 0.05


LightAttentionFFN:

  in_seq_len: 20
  hidden_layers: !HiddenLayersExact
    - 32
  dropout_rate: 0.25

  test_parameters:
    hidden_layers: !HiddenLayers
      num_layers: !RangeInt
        min: 1
        max: 3
      layer_size_percentage: !RangeFloat
        min: 0.05
        max: 0.6
        step: 0.05


# -------------------- Reduced FFN Models --------------------

MaxPoolFastKAN:

  in_seq_len: 1023 # * embeddings_dim=1024 = 1_047_552 (too much that's why dimension reduction)
  hidden_layers: !HiddenLayersRelative
    num_layers: 1
    relative_size: 0.04
  reduced_seq_len: 20
  grid_diff: 2.0
  num_grids: 5

  test_parameters:
    hidden_layers: !HiddenLayers
      num_layers: !RangeInt
        min: 1
        max: 3
      layer_size_percentage: !RangeFloat
        min: 0.02
        max: 0.24
        step: 0.02
    reduced_seq_len: !RangeInt
      min: 10
      max: 60
      step: 10
    grid_diff: !RangeFloat
      min: 1.0
      max: 3.0
      step: 0.5
    num_grids: !RangeInt
      min: 3
      max: 6
      step: 1


MaxPoolMLP:

  in_seq_len: 1023
  hidden_layers: !HiddenLayersRelative
    num_layers: 2
    relative_size: 0.4
  reduced_seq_len: 20

  test_parameters:
    hidden_layers: !HiddenLayers
      num_layers: !RangeInt
        min: 1
        max: 6
      layer_size_percentage: !RangeFloat
        min: 0.30
        max: 0.80
        step: 0.10
    reduced_seq_len: !RangeInt
      min: 10
      max: 60
      step: 10


AvgPoolFastKAN:

  in_seq_len: 1023
  hidden_layers: !HiddenLayersRelative
    num_layers: 1
    relative_size: 0.04
  reduced_seq_len: 20
  grid_diff: 2.0
  num_grids: 5

  test_parameters:
    hidden_layers: !HiddenLayers
      num_layers: !RangeInt
        min: 1
        max: 3
      layer_size_percentage: !RangeFloat
        min: 0.02
        max: 0.24
        step: 0.02
    reduced_seq_len: !RangeInt
      min: 10
      max: 60
      step: 10
    grid_diff: !RangeFloat
      min: 1.0
      max: 3.0
      step: 0.5
    num_grids: !RangeInt
      min: 3
      max: 6
      step: 1


AvgPoolMLP:

  in_seq_len: 1023
  hidden_layers: !HiddenLayersRelative
    num_layers: 2
    relative_size: 0.4
  reduced_seq_len: 20

  test_parameters:
    hidden_layers: !HiddenLayers
      num_layers: !RangeInt
        min: 1
        max: 6
      layer_size_percentage: !RangeFloat
        min: 0.30
        max: 0.80
        step: 0.10
    reduced_seq_len: !RangeInt
      min: 10
      max: 60
      step: 10


LinearFastKAN:

  in_seq_len: 1023
  hidden_layers: !HiddenLayersRelative
    num_layers: 1
    relative_size: 0.04
  reduced_seq_len: 20
  grid_diff: 2.0
  num_grids: 5

  test_parameters:
    hidden_layers: !HiddenLayers
      num_layers: !RangeInt
        min: 1
        max: 3
      layer_size_percentage: !RangeFloat
        min: 0.02
        max: 0.24
        step: 0.02
    reduced_seq_len: !RangeInt
      min: 10
      max: 60
      step: 10
    grid_diff: !RangeFloat
      min: 1.0
      max: 3.0
      step: 0.5
    num_grids: !RangeInt
      min: 3
      max: 6
      step: 1


LinearMLP:

  in_seq_len: 1023
  hidden_layers: !HiddenLayersRelative
    num_layers: 2
    relative_size: 0.4
  reduced_seq_len: 20

  test_parameters:
    hidden_layers: !HiddenLayers
      num_layers: !RangeInt
        min: 1
        max: 6
      layer_size_percentage: !RangeFloat
        min: 0.30
        max: 0.80
        step: 0.10
    reduced_seq_len: !RangeInt
      min: 10
      max: 60
      step: 10


AttentionFastKAN:

  in_seq_len: 1023
  hidden_layers: !HiddenLayersRelative
    num_layers: 1
    relative_size: 0.04
  # hidden_layers: !HiddenLayersExact
  #  - 768
  reduced_seq_len: 20
  grid_diff: 2.0
  num_grids: 5
  num_heads: 8

  test_parameters:
    hidden_layers: !HiddenLayers
      num_layers: !RangeInt
        min: 1
        max: 3
      layer_size_percentage: !RangeFloat
        min: 0.02
        max: 0.24
        step: 0.02
    reduced_seq_len: !RangeInt
      min: 10
      max: 60
      step: 10
    num_heads: !Categorical
      choices: [ 4, 8, 16 ]
    grid_diff: !RangeFloat
      min: 1.0
      max: 3.0
      step: 0.5
    num_grids: !RangeInt
      min: 3
      max: 6
      step: 1


AttentionMLP:

  in_seq_len: 1023
  hidden_layers: !HiddenLayersRelative
    num_layers: 2
    relative_size: 0.4
  reduced_seq_len: 20
  num_heads: 8

  test_parameters:
    hidden_layers: !HiddenLayers
      num_layers: !RangeInt
        min: 1
        max: 6
      layer_size_percentage: !RangeFloat
        min: 0.30
        max: 0.80
        step: 0.10
    reduced_seq_len: !RangeInt
      min: 10
      max: 60
      step: 10
    num_heads: !Categorical
      choices: [ 4, 8, 16 ]


PositionalFastKAN:

  in_seq_len: 1023
  hidden_layers: !HiddenLayersRelative
    num_layers: 1
    relative_size: 0.04
  # hidden_layers: !HiddenLayersExact
  #  - 384
  reduced_seq_len: 20
  reduced_channels: 512
  kernel_size: 3
  weights_scalar: 2.0
  grid_diff: 2.0
  num_grids: 5

  test_parameters:
    hidden_layers: !HiddenLayers
      num_layers: !RangeInt
        min: 1
        max: 3
      layer_size_percentage: !RangeFloat
        min: 0.02
        max: 0.24
        step: 0.02
    reduced_seq_len: !RangeInt
      min: 10
      max: 60
      step: 10
    reduced_channels: !RangeInt
      min: 256
      max: 1024
      step: 64
    kernel_size: !Categorical
      choices: [ 3, 5, 7 ]
    weights_scalar: !RangeFloat
      min: 1.0
      max: 5.0
      step: 1.0
    grid_diff: !RangeFloat
      min: 1.0
      max: 3.0
      step: 0.5
    num_grids: !RangeInt
      min: 3
      max: 6
      step: 1


PositionalMLP:

  in_seq_len: 1023
  hidden_layers: !HiddenLayersRelative
    num_layers: 2
    relative_size: 0.4
  reduced_seq_len: 20
  reduced_channels: 512
  kernel_size: 3
  weights_scalar: 2.0

  test_parameters:
    hidden_layers: !HiddenLayers
      num_layers: !RangeInt
        min: 1
        max: 6
      layer_size_percentage: !RangeFloat
        min: 0.30
        max: 0.80
        step: 0.10
    reduced_seq_len: !RangeInt
      min: 10
      max: 60
      step: 10
    reduced_channels: !RangeInt
      min: 256
      max: 1024
      step: 64
    kernel_size: !Categorical
      choices: [3, 5, 7]
    weights_scalar: !RangeFloat
      min: 1.0
      max: 5.0
      step: 1.0


UNetFastKAN:

  in_seq_len: 1023
  hidden_layers: !HiddenLayersRelative
    num_layers: 1
    relative_size: 0.04
  reduced_seq_len: 20
  reduced_channels: 512
  kernel_size: 3
  grid_diff: 2.0
  num_grids: 5

  test_parameters:
    hidden_layers: !HiddenLayers
      num_layers: !RangeInt
        min: 1
        max: 3
      layer_size_percentage: !RangeFloat
        min: 0.02
        max: 0.24
        step: 0.02
    reduced_seq_len: !RangeInt
      min: 10
      max: 60
      step: 10
    reduced_channels: !RangeInt
      min: 256
      max: 1024
      step: 64
    kernel_size: !Categorical
      choices: [ 3, 5, 7 ]
    grid_diff: !RangeFloat
      min: 1.0
      max: 3.0
      step: 0.5
    num_grids: !RangeInt
      min: 3
      max: 6
      step: 1


UNetMLP:

  in_seq_len: 1023
  hidden_layers: !HiddenLayersRelative
    num_layers: 2
    relative_size: 0.4
  reduced_seq_len: 20
  reduced_channels: 512
  kernel_size: 3

  test_parameters:
    hidden_layers: !HiddenLayers
      num_layers: !RangeInt
        min: 1
        max: 6
      layer_size_percentage: !RangeFloat
        min: 0.30
        max: 0.80
        step: 0.10
    reduced_seq_len: !RangeInt
      min: 10
      max: 60
      step: 10
    reduced_channels: !RangeInt
      min: 256
      max: 1024
      step: 64
    kernel_size: !Categorical
      choices: [3, 5, 7]


# -------------------- Other Models --------------------

AttentionLstmHybridFastKAN:

  in_seq_len: 1023 # Input sequence length from ProtT5 embeddings
  hidden_layers: !HiddenLayersRelative
    num_layers: 1
    relative_size: 0.04
  attention_num_heads: 8
  lstm_hidden_size: 256
  lstm_num_layers: 2
  dropout1_rate: 0.25
  dropout2_rate: 0.25
  grid_diff: 2.0
  num_grids: 5

  test_parameters:
    hidden_layers: !HiddenLayers
      num_layers: !RangeInt
        min: 1
        max: 3
      layer_size_percentage: !RangeFloat
        min: 0.02
        max: 0.24
        step: 0.02
    attention_num_heads: !Categorical
      choices: [4, 8, 16]
    lstm_hidden_size: !RangeInt
      min: 128
      max: 512
      step: 32
    lstm_num_layers: !RangeInt
      min: 1
      max: 4
    grid_diff: !RangeFloat
      min: 2.0
      max: 2.0
      step: 0.0
    num_grids: !RangeInt
      min: 4
      max: 6
      step: 1


LstmAttentionReductionHybridFastKAN:
  # Simplified the model configuration due to clear signs of overfitting.

  in_seq_len: 1023 # Input sequence length from ProtT5 embeddings
  hidden_layers: !HiddenLayersRelative
    num_layers: 1
    relative_size: 0.04
  reduced_seq_len: 20
  lstm_hidden_size: 128
  lstm_num_layers: 2
  dropout_rate: 0.40
  num_heads: 8
  grid_diff: 2.0
  num_grids: 5

  test_parameters:
    hidden_layers: !HiddenLayers
      num_layers: !RangeInt
        min: 1
        max: 3
      layer_size_percentage: !RangeFloat
        min: 0.02
        max: 0.24
        step: 0.02
    reduced_seq_len: !RangeInt
      min: 10
      max: 60
      step: 10
    lstm_hidden_size: !RangeInt
      min: 128
      max: 512
      step: 32
    lstm_num_layers: !RangeInt
      min: 1
      max: 4
    dropout_rate: !RangeFloat
      min: 0.1
      max: 0.5
    num_heads: !Categorical
      choices: [ 4, 8, 16 ]
    grid_diff: !RangeFloat
      min: 2.0
      max: 2.0
      step: 0.0
    num_grids: !RangeInt
      min: 4
      max: 6
      step: 1


LightAttention:
  # Configuration exactly as described in the paper "Light attention predicts protein location from the language of life".

  in_seq_len: 1023  # Input sequence length from ProtT5 embeddings
  hidden_layers: !HiddenLayersExact
    - 32
  kernel_size: 9
  conv_dropout_rate: 0.25
  ffn_dropout_rate: 0.25

  test_parameters:
    hidden_layers: !HiddenLayers
      num_layers: !RangeInt
        min: 1
        max: 4
      layer_size_percentage: !RangeFloat
        min: 0.2
        max: 0.6
        step: 0.1
    kernel_size: !RangeInt
      min: 3
      max: 11
      step: 2
    conv_dropout_rate: !RangeFloat
      min: 0.1
      max: 0.5
      step: 0.1
    ffn_dropout_rate: !RangeFloat
      min: 0.1
      max: 0.5
      step: 0.1


LightAttentionFastKAN:
  # Settings similar to LightAttention (for comparison)

  in_seq_len: 1023  # Input sequence length from ProtT5 embeddings
  hidden_layers: !HiddenLayersExact
    - 128 # Going from 2048 (2 * input_channels) to 32 is a massive reduction.
  kernel_size: 9
  conv_dropout_rate: 0.25
  grid_diff: 1.5  # smaller window for less overfitting
  num_grids: 5

  test_parameters:
    hidden_layers: !HiddenLayers
      num_layers: !RangeInt
        min: 1
        max: 3
      layer_size_percentage: !RangeFloat
        min: 0.02
        max: 0.24
        step: 0.02
    kernel_size: !RangeInt
      min: 3
      max: 11
      step: 2
    conv_dropout_rate: !RangeFloat
      min: 0.1
      max: 0.5
    grid_diff: !RangeFloat
      min: 2.0
      max: 2.0
      step: 0.0
    num_grids: !RangeInt
      min: 4
      max: 6
      step: 1